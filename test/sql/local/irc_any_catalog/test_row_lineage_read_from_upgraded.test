# name: test/sql/local/irc_any_catalog/test_row_lineage_read_from_upgraded.test
# description: test integration with iceberg catalog read
# group: [irc_any_catalog]

require-env ICEBERG_SERVER_AVAILABLE

require-env SECRETS_CREATED_AND_CATALOG_ATTACHED

require avro

require parquet

require iceberg

require httpfs

require core_functions

# Do not ignore 'HTTP' error messages!
set ignore_error_messages

statement ok
set enable_logging=true

statement ok
set logging_level='debug'

loop i 0 7

statement ok
set variable snapshot${i} = (
	select {
		'snapshot_id': snapshot_id,
		'timestamp_ms': timestamp_ms
	} from iceberg_snapshots(
		my_datalake.default.row_lineage_test_upgraded_insert
	) order by timestamp_ms
	offset ${i} limit 1
);

endloop

query IIII
select _last_updated_sequence_number, _row_id, id, data from my_datalake.default.row_lineage_test_upgraded_insert AT (VERSION=>getvariable('snapshot0').snapshot_id) order by id;
----
1	NULL	1	a
1	NULL	2	b
1	NULL	3	c
1	NULL	4	d
1	NULL	5	e

query IIII
select _last_updated_sequence_number, _row_id, id, data from my_datalake.default.row_lineage_test_upgraded_insert AT (VERSION=>getvariable('snapshot1').snapshot_id) order by id;
----
1	NULL	1	a
2	NULL	2	b_u1
1	NULL	3	c
2	NULL	4	d_u1
1	NULL	5	e

query IIII
select _last_updated_sequence_number, _row_id, id, data from my_datalake.default.row_lineage_test_upgraded_insert AT (VERSION=>getvariable('snapshot2').snapshot_id) order by id;
----
1	NULL	1	a
2	NULL	2	b_u1
2	NULL	4	d_u1

# Insert 2 new rows (because of various rewrites this starts with a much higher row_id)
query IIII
select _last_updated_sequence_number, _row_id, id, data from my_datalake.default.row_lineage_test_upgraded_insert AT (VERSION=>getvariable('snapshot3').snapshot_id) order by id;
----
1	NULL	1	a
2	NULL	2	b_u1
2	NULL	4	d_u1
4	NULL	6	f
4	NULL	7	g

# UPDATE row_ids 0 and 11
query IIII
select _last_updated_sequence_number, _row_id, id, data from my_datalake.default.row_lineage_test_upgraded_insert AT (VERSION=>getvariable('snapshot4').snapshot_id) order by id;
----
5	NULL	1	replaced
2	NULL	2	b_u1
2	NULL	4	d_u1
5	NULL	6	replaced
4	NULL	7	g

# DELETE row_id 12
query IIII
select _last_updated_sequence_number, _row_id, id, data from my_datalake.default.row_lineage_test_upgraded_insert AT (VERSION=>getvariable('snapshot5').snapshot_id) order by id;
----
5	NULL	1	replaced
2	NULL	2	b_u1
2	NULL	4	d_u1
5	NULL	6	replaced

# INSERT row_id 16 (again, because of various rewrites this starts with a much higher row_id)
query IIII
select _last_updated_sequence_number, _row_id, id, data from my_datalake.default.row_lineage_test_upgraded_insert AT (VERSION=>getvariable('snapshot6').snapshot_id) order by id;
----
5	NULL	1	replaced
2	NULL	2	b_u1
2	NULL	4	d_u1
5	NULL	6	replaced
7	NULL	7	g_new

# FIXME: need duckdb 1.5.1 to "fix" this, so we can disable pushdown for _row_id
# Latest state of the table, after upgrading to V3 and creating a new snapshot
query II
select id, data from my_datalake.default.row_lineage_test_upgraded_insert where _row_id is NULL order by id;
----
2	replaced_again
6	replaced_again
