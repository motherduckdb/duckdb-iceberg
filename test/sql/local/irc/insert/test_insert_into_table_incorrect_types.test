# name: test/sql/local/irc/insert/test_insert_into_table_incorrect_types.test
# description: test integration with iceberg catalog read
# group: [insert]

require-env ICEBERG_SERVER_AVAILABLE

require avro

require parquet

require iceberg

require httpfs

# Do not ignore 'HTTP' error messages!
set ignore_error_messages

statement ok
CALL enable_logging('HTTP');

statement ok
set logging_level='debug';

statement ok
CREATE SECRET (
    TYPE S3,
    KEY_ID 'admin',
    SECRET 'password',
    ENDPOINT '127.0.0.1:9000',
    URL_STYLE 'path',
    USE_SSL 0
);


statement ok
ATTACH '' AS my_datalake (
    TYPE ICEBERG,
    CLIENT_ID 'admin',
    CLIENT_SECRET 'password',
    ENDPOINT 'http://127.0.0.1:8181'
);

statement ok
create schema if not exists my_datalake.default;

statement ok
drop table if exists my_datalake.default.test;

statement ok
create table my_datalake.default.test (int_col int, long_col long, string_col varchar);

statement error
insert into my_datalake.default.test values (4294967295::UINT32, null, null);
----
<REGEX>:.*Conversion Error.*out of range for the destination type INT32.*

# insert a tinytint
statement ok
insert into my_datalake.default.test values (8::TINYINT, null, null);

# there is only 1 datafile right now
statement ok
set variable data_file = (select file_path  from iceberg_metadata(my_datalake.default.test) limit 1);

# query the column_type of the parquet file, make sure it is integer and not tinyint.
query I
select column_type from (describe (from read_parquet(getvariable('data_file')))) where column_name = 'int_col';
----
INTEGER





