# name: test/sql/local/irc/irc_catalog_read_deletes.test
# description: test integration with iceberg catalog read
# group: [irc]

require-env ICEBERG_SERVER_AVAILABLE

require avro

require parquet

require iceberg

require httpfs

# Do not ignore 'HTTP' error messages!
set ignore_error_messages

statement ok
CREATE SECRET (
    TYPE S3,
    KEY_ID 'admin',
    SECRET 'password',
    ENDPOINT '127.0.0.1:9000',
    URL_STYLE 'path',
    USE_SSL 0
);


statement ok
ATTACH 'demo' AS my_datalake (
	TYPE ICEBERG,
	CLIENT_ID 'admin',
	CLIENT_SECRET 'password',
	ENDPOINT 'http://127.0.0.1:8181'
);

# TODO verify the catalog has deletes (rest catalog stores data differently from local catalog)
query I nosort results_1
select sum(l_suppkey), min(l_suppkey), max(l_suppkey) from my_datalake.default.lineitem_001_deletes;

query I nosort results_1
select sum(l_suppkey), min(l_suppkey), max(l_suppkey) from read_parquet('data/generated/intermediates/spark-rest/lineitem_001_deletes/last/data.parquet/*.parquet');

# Verify Deletes
# joins with a table that has deletes.
query I nosort results_2
select l1_deletes.l_orderkey, count(*) count from
  my_datalake.default.lineitem_sf1_deletes l1_deletes,
  my_datalake.default.lineitem_sf_01_no_deletes l2_no_deletes
where l1_deletes.l_partkey = l2_no_deletes.l_partkey
group by l1_deletes.l_orderkey
order by l1_deletes.l_orderkey, count
limit 10;
----


query I nosort results_2
select l1_deletes.l_orderkey, count(*) count from
    read_parquet('data/generated/intermediates/spark-rest/lineitem_sf1_deletes/last/data.parquet/*.parquet') l1_deletes,
    read_parquet('data/generated/intermediates/spark-rest/lineitem_sf_01_no_deletes/last/data.parquet/*.parquet') l2_no_deletes
where l1_deletes.l_partkey = l2_no_deletes.l_partkey
group by l1_deletes.l_orderkey
order by l1_deletes.l_orderkey, count
limit 10
;

# Verify a single delete
query IIII nosort result_3
select l_orderkey, l_partkey, l_suppkey, l_quantity from my_datalake.default.lineitem_sf_01_1_delete order by l_partkey, l_orderkey limit 10;
----

query IIII nosort result_3
select l_orderkey, l_partkey, l_suppkey, l_quantity from read_parquet('data/generated/intermediates/spark-rest/lineitem_sf_01_1_delete/last/data.parquet/*.parquet') order by l_partkey, l_orderkey limit 10;
----

query I
select count(*) from my_datalake.default.lineitem_sf_01_1_delete where l_orderkey=10053 and l_partkey = 77;
----
0

query I
select count(*) from read_parquet('data/generated/intermediates/spark-rest/lineitem_sf_01_1_delete/last/data.parquet/*.parquet') where l_orderkey=10053 and l_partkey = 77;
----
0


# Verify reading from large partitioned table
# add tests for partitioned tables.
query II nosort result_4
select l_shipmode, count(*) count from my_datalake.default.lineitem_partitioned_l_shipmode group by l_shipmode order by count;
----

query II nosort result_4
select l_shipmode, count(*) count from read_parquet('data/generated/intermediates/spark-rest/lineitem_partitioned_l_shipmode/last/data.parquet/*.parquet') group by l_shipmode order by count;
----


# Verify reading from large partitioned table with deletes
query II nosort result_5
select l_shipmode, count(*) count from my_datalake.default.lineitem_partitioned_l_shipmode_deletes group by l_shipmode order by count;
----

query II nosort result_5
select l_shipmode, count(*) count from read_parquet('data/generated/intermediates/spark-rest/lineitem_partitioned_l_shipmode_deletes/last/data.parquet/*.parquet') group by l_shipmode order by count;
----


