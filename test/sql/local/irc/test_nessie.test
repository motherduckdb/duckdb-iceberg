# name: test/sql/local/irc/test_nessie.test
# description: test nessie integration
# group: [irc]

require-env NESSIE_SERVER_AVAILABLE

require avro

require parquet

require iceberg

require httpfs

require aws

set ignore_error_messages

statement ok
CREATE SECRET storage_secret (
    TYPE S3,
    KEY_ID 'minioadmin',
    SECRET 'minioadmin',
    ENDPOINT '127.0.0.1:9002',
    URL_STYLE 'path',
    USE_SSL 0
);

statement ok
create secret my_secret (
    TYPE ICEBERG,
    CLIENT_ID 'client1',
    CLIENT_SECRET 's3cr3t',
    OAUTH2_SCOPE 'catalog sign',
    OAUTH2_SERVER_URI 'http://127.0.0.1:8080/realms/iceberg/protocol/openid-connect/token'
);

statement ok
attach '' as my_datalake (
    type ICEBERG,
    ENDPOINT 'http://127.0.0.1:19120/iceberg/',
    ACCESS_DELEGATION_MODE 'none',
    SECRET 'my_secret'
);

statement ok
create schema if not exists my_datalake.default;

statement ok
create table if not exists my_datalake.default.t1 (a int);

statement ok
insert into my_datalake.default.t1 select range a from range(10);

statement ok
create table if not exists my_datalake.default.t2 as select range a from range(1000);

statement ok
drop table if exists my_datalake.default.t2;

statement ok
drop table if exists my_datalake.default.t1;

statement ok
detach my_datalake;

statement ok
drop secret storage_secret;

# test with nessie vending credentials
# vending credentials is default
statement ok
attach '' as my_datalake (
    type ICEBERG,
    ENDPOINT 'http://127.0.0.1:19120/iceberg/',
    SECRET 'my_secret'
);

statement ok
call enable_logging();

statement ok
create schema if not exists my_datalake.new_schema;

statement ok
drop table if exists my_datalake.new_schema.table_1;

statement error
create table my_datalake.new_schema.table_1 as select range a from range(100);
----
<REGEX>:.*HTTP Error: Unable to connect.*Forbidden.*403.*

query I
select message from duckdb_logs() where message like '%Failed%';
----
Failed to create valid secret from Vendend Credentials for table 'table_1'

statement ok
drop schema my_datalake.new_schema;
